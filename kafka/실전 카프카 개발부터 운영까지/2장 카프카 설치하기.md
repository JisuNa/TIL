# 2장 카프카 설치하기

## 브로커 설정

### 핵심 브로커 매개변수

**broker.id**

브로커는 정수값 식별자를 갖는다.<br/>
브로커별 정수값이 달라야한다.<br/>
호스트별 고정된 값을 사용 권장<br/>

**listeners**

TCP 9092<br/>
listener.security.protocol.map 설정 잡기<br/>
0.0.0.0 으로하면 모든 네트워크에서 연결이 가능<br/>

**zookeeper.connect**

브로커의 메타데이터가 저장되는 주키퍼의 위치를 가르킨다.<br/>
`{호스트 이름}:{포트}/{경로}`

**log.dirs**

카프카는 모든 메시지를 로그 세그먼트 단위로 log.dir 설정에 지정된 디렉토리에 저장

**num.recovery.threads.per.data.dir**

스레드 풀을 사용해서 로그 세그먼트를 관리한다.

스레드 풀 작업
- 브로커 정상 시작할 때, 각 파티션의 로그 세그먼트 파일을 연다.
- 브로커 장애 발생 후 다시 시작됐을 때, 각 파티션의 로그 세그먼트를 검사하고 잘못된 부분은 삭제한다.
- 브로커가 종료할 때, 로그 세그먼트를 정상적으로 닫는다.

기본적으로 로그 디렉토리와 스레드는 1:1로 사용한다.<br/>
스레드들은 브로커 시작,종료에만 사용되어 작업을 병렬 처리할 수 있게 많은 수의 스레드를 할당하는게 좋다.

이 설정을 어떻게 잡아 주느냐에 따라 언클린 셧다운 이후 복구를 위한 재시작 시작이 몇 시간씩 차이가 날 수도 있다.

예를 들어.
number.recovery.threads.per.data.dir : 8<br/>
log.dirs : 지정경로 3개<br/>
이 경우 전체 스레드 수는 24개가 된다.<br/>

**auto.create.topics.enable**

브로커가 토픽을 자동생성할 수 있지만, 바람직하지 않다. 존재여부를 확인할 방법이 없다.

토픽 생성을 관리하고 싶으면, 해당 설정을 false로 설정한다.

**auto.leader.rebalance.enable**

모든 토픽의 리더 역할이 하나의 브로커에 집중되면 카프카 클러스터의 균형이 깨질 수 있다. 이 설정을 활성화하면 리더 역할이 균등하게 분산된다.<br/>
이 설정을 키면 파티션의 분포 상태를 주기적으로 백그라운드로 확인한다.

**delete.topic.enable**

토픽 삭제 기능을 막는다.


### 토픽별 기본값

**num.partitions**

새 토픽이 생성될 때 몇 개의 파티션을 가지게 되는지 결정한다.<br/>
토픽의 파티션 개수는 늘릴 수는 있고 줄일 수는 없다.<br/>
많은 유저들은 토픽당 파티션 개수를 브로커의 수와 맞추거나 배수로 설정한다.<br/>
이렇게 하면 파티션이 브로커들 사이에 고르게 분산된다.<br/>

**default.replication.factor**

자동 토픽 생성이 활성화된 경우 복제 팩터를 결정한다.<br/>
min.insync.replicas 설정값보다 최소 +1이상 크게 잡아주고, 서버 성능이 좋으면 +2 이상<br/>
최소 3개의 레플리카를 가져야지 장애가 발생하지 않는다.<br/>

**log.retention.ms**	

메시지 보존 기간 설정이다.<br/>
아래 시간별로 설정이 가능하며, ms > minutes > hours 순으로 우선순위다.<br/>
log.retention.hour = 168시간(default)<br/>
log.retention.minutes<br/>
log.retention.ms<br/>

**log.retention.bytes**	

메시지 보존 용량 설정이다.<br/>
파티션이 8개이고 1GB로 잡혀있으면 토픽의 최대 저장 용량은 8GB<br/>
모든 보존 기능은 파티션 단위 (토픽 단위X)<br/>
-1은 영구 보존<br/>

**log.segment.bytes**	

로그 세그먼트의 크기가 모두 사용되면 기존 세그먼트는 닫고 새로운 세그먼트를 연다. 로그 세그먼트는 닫히기 전까지 만료와 삭제대상이 되지 않는다.<br/>
토픽 메시지가 뜸하면 세그먼트가 닫히기까지 몇일이 걸릴 수 있다.

**log.roll.ms**

파일이 닫혀야 할 때까지 기다리는 시간을 지정하는 설정

**min.insync.replicas**

2로 잡아주면 2개의 레플리카가 최신상태로 퓨로듀서와 동기화된다.<br/>
ack=’all’ 설정과 함께 사용하면 메시지 유실을 방지할 수 있다.<br/>
몇 개의 메시지 유실되도 상관없고, 높은 처리량을 받아내야 한다면 1로 사용하는 것을 권장한다.<br/>

**message.max.bytes**

쓸 수 있는 메시지 최대 크기를 설정한다.<br/>
컨슈머의 fetch.message.max.bytes와 값이 맞아야한다.<br/>
작을 경우 읽기 진행이 멈출 수 있다.

### 파티션 수 결정 요소

- 목표 처리량은 어느정도 인가?
- 단일 파티션에 최대 읽기 처리량의 목표는 어느정도 인가?
  하나의 파티션은 하나의 컨슈머만 읽을 수 있다.

## 하드웨어 선택

### 디스크 용량

브로커가 하루에 1TB를 트래픽을 받을 것으로 예상하고 1주일간 보관한다면 브로커는 로그 세그먼트를 저장하기 위한 7TB가  필요하다.

트래픽 변동/증가를 대비해서 최소 10% 오버헤드를 고려해야 한다.

### 메모리

컨슈머는 프로듀서가 막 추가한 메시지를 바로 읽어오는 것이 일반적이다.

최적의 작동은 시스템의 페이지 캐시에 저장되어 있는 메시지를 컨슈머가 읽는게 좋다.

페이지 캐시로 사용할 수 있는 메모리를 더 할당해주면 컨슈머의 성능을 향상시킬 수 있다.

초당 150,000개의 메시지에 200MB의 데이터 속도를 처리하는 브로커도 5GB 힙으로 충분하다.

다른 어플리케이션과 같이 쓰지 않는게 카프카 성능에 좋다.(페이지 캐시를 나눠쓰지 않기 때문)

## 클라우드에서 카프카

### AWS

지연이 매우 낮아야하면 로컬 SSD가 장착된 I/O 최적화 인스턴스가 필요

그게 아니면 EBS

m4를 쓰면 보존 기한은 더 늘려서 좋지만 디스크라 처리량이 줄어든다.

r3ㄹ르 쓰면 SSD 써서 처리량은 올라가겠지만 보존 가능한 데이터 양에 제한이 걸린다.

두가지 장점을 다 가지는 i2/d2는 비싸다
