# 2장 카프카 설치하기

## 브로커 설정

### 핵심 브로커 매개변수

**broker.id**

브로커는 정수값 식별자를 갖는다.<br/>
브로커별 정수값이 달라야한다.<br/>
호스트별 고정된 값을 사용 권장<br/>

**listeners**

TCP 9092<br/>
listener.security.protocol.map 설정 잡기<br/>
0.0.0.0 으로하면 모든 네트워크에서 연결이 가능<br/>

**zookeeper.connect**

브로커의 메타데이터가 저장되는 주키퍼의 위치를 가르킨다.<br/>
`{호스트 이름}:{포트}/{경로}`

**log.dirs**

카프카는 모든 메시지를 로그 세그먼트 단위로 log.dir 설정에 지정된 디렉토리에 저장

**num.recovery.threads.per.data.dir**

스레드 풀을 사용해서 로그 세그먼트를 관리한다.

스레드 풀 작업
- 브로커 정상 시작할 때, 각 파티션의 로그 세그먼트 파일을 연다.
- 브로커 장애 발생 후 다시 시작됐을 때, 각 파티션의 로그 세그먼트를 검사하고 잘못된 부분은 삭제한다.
- 브로커가 종료할 때, 로그 세그먼트를 정상적으로 닫는다.

기본적으로 로그 디렉토리와 스레드는 1:1로 사용한다.<br/>
스레드들은 브로커 시작,종료에만 사용되어 작업을 병렬 처리할 수 있게 많은 수의 스레드를 할당하는게 좋다.

이 설정을 어떻게 잡아 주느냐에 따라 언클린 셧다운 이후 복구를 위한 재시작 시작이 몇 시간씩 차이가 날 수도 있다.

예를 들어.
number.recovery.threads.per.data.dir : 8<br/>
log.dirs : 지정경로 3개<br/>
이 경우 전체 스레드 수는 24개가 된다.<br/>

**auto.create.topics.enable**

브로커가 토픽을 자동생성할 수 있지만, 바람직하지 않다. 존재여부를 확인할 방법이 없다.

토픽 생성을 관리하고 싶으면, 해당 설정을 false로 설정한다.

**auto.leader.rebalance.enable**

모든 토픽의 리더 역할이 하나의 브로커에 집중되면 카프카 클러스터의 균형이 깨질 수 있다. 이 설정을 활성화하면 리더 역할이 균등하게 분산된다.<br/>
이 설정을 키면 파티션의 분포 상태를 주기적으로 백그라운드로 확인한다.

**delete.topic.enable**

토픽 삭제 기능을 막는다.


### 토픽별 기본값

**num.partitions**

새 토픽이 생성될 때 몇 개의 파티션을 가지게 되는지 결정한다.<br/>
토픽의 파티션 개수는 늘릴 수는 있고 줄일 수는 없다.<br/>
많은 유저들은 토픽당 파티션 개수를 브로커의 수와 맞추거나 배수로 설정한다.<br/>
이렇게 하면 파티션이 브로커들 사이에 고르게 분산된다.<br/>

**default.replication.factor**

자동 토픽 생성이 활성화된 경우 복제 팩터를 결정한다.<br/>
min.insync.replicas 설정값보다 최소 +1이상 크게 잡아주고, 서버 성능이 좋으면 +2 이상<br/>
최소 3개의 레플리카를 가져야지 장애가 발생하지 않는다.<br/>

**log.retention.ms**	

메시지 보존 기간 설정이다.<br/>
아래 시간별로 설정이 가능하며, ms > minutes > hours 순으로 우선순위다.<br/>
log.retention.hour = 168시간(default)<br/>
log.retention.minutes<br/>
log.retention.ms<br/>

**log.retention.bytes**	

메시지 보존 용량 설정이다.<br/>
파티션이 8개이고 1GB로 잡혀있으면 토픽의 최대 저장 용량은 8GB<br/>
모든 보존 기능은 파티션 단위 (토픽 단위X)<br/>
-1은 영구 보존<br/>

**log.segment.bytes**	

로그 세그먼트의 크기가 모두 사용되면 기존 세그먼트는 닫고 새로운 세그먼트를 연다. 로그 세그먼트는 닫히기 전까지 만료와 삭제대상이 되지 않는다.<br/>
토픽 메시지가 뜸하면 세그먼트가 닫히기까지 몇일이 걸릴 수 있다.

**log.roll.ms**

파일이 닫혀야 할 때까지 기다리는 시간을 지정하는 설정

**min.insync.replicas**

2로 잡아주면 2개의 레플리카가 최신상태로 퓨로듀서와 동기화된다.<br/>
ack=’all’ 설정과 함께 사용하면 메시지 유실을 방지할 수 있다.<br/>
몇 개의 메시지 유실되도 상관없고, 높은 처리량을 받아내야 한다면 1로 사용하는 것을 권장한다.<br/>

**message.max.bytes**

쓸 수 있는 메시지 최대 크기를 설정한다.<br/>
컨슈머의 fetch.message.max.bytes와 값이 맞아야한다.<br/>
작을 경우 읽기 진행이 멈출 수 있다.

### 파티션 수 결정 요소

- 목표 처리량은 어느정도 인가?
- 단일 파티션에 최대 읽기 처리량의 목표는 어느정도 인가?
  하나의 파티션은 하나의 컨슈머만 읽을 수 있다.

## 하드웨어 선택

### 디스크 용량

브로커가 하루에 1TB를 트래픽을 받을 것으로 예상하고 1주일간 보관한다면 브로커는 로그 세그먼트를 저장하기 위한 7TB가  필요하다.

트래픽 변동/증가를 대비해서 최소 10% 오버헤드를 고려해야 한다.

### 메모리

컨슈머는 프로듀서가 막 추가한 메시지를 바로 읽어오는 것이 일반적이다.

최적의 작동은 시스템의 페이지 캐시에 저장되어 있는 메시지를 컨슈머가 읽는게 좋다.

페이지 캐시로 사용할 수 있는 메모리를 더 할당해주면 컨슈머의 성능을 향상시킬 수 있다.

초당 150,000개의 메시지에 200MB의 데이터 속도를 처리하는 브로커도 5GB 힙으로 충분하다.

다른 어플리케이션과 같이 쓰지 않는게 카프카 성능에 좋다.(페이지 캐시를 나눠쓰지 않기 때문)

## 클라우드에서 카프카

### AWS

지연이 매우 낮아야하면 로컬 SSD가 장착된 I/O 최적화 인스턴스가 필요

그게 아니면 EBS

m4를 쓰면 보존 기한은 더 늘려서 좋지만 디스크라 처리량이 줄어든다.

r3ㄹ르 쓰면 SSD 써서 처리량은 올라가겠지만 보존 가능한 데이터 양에 제한이 걸린다.

두가지 장점을 다 가지는 i2/d2는 비싸다

## 카프카 클러스터 설정하기

### 장점

가장 큰 이점은 부하를 다수의 서버로 확장할 수 있다.

복제를 사용함으로 데이터 유실을 방지할 수 있다.

### 브로커 개수

카프카 클러스터의 적절한 크기를 결정하는 요소

- 디스크 용량
- 브로커당 레플리카 용량
- CPU 용량
- 네트워크 용량

**예시.1** 10TB의 데이터 저장. 1개의 브로커에서 저장할 수 있는 용량 2TB 라고 가정.

클러스터는 최소 5개의 브로커가 필요.
복제 팩터를 증가시킬 경우 배수로 필요하다.
복제 팩터: 2 → 10대의 브로커
복제 팩터: 3 → 15대의 브로커


디스크 처리량이나 시스템 메모리가 부족하여 발생하는 성능 문제는 브로커를 추가하여 확장할 것.

### 브로커 설정

다수의 브로커가 하나의 클러스터를 이루게 하는 설정

- 동일한 zookeeper.connect 설정 값
- 동일한 [broker.id](http://broker.id) 설정 값

### 운영체제 튜닝하기

카프카 성능을 끌어올릴 수 있는 튜닝

- 가상 메모리
- 네트워크 서브시스템
- 로그 세그먼트가 저장되는 디스크 마운트

1. 가상메모리

일반적으로 리눅스의 가상 메모리 시스템은 시스템 부하에 따라 자동으로 메모리 사용량을 조절한다.

카프카는 부하 특성에 맞게 스왑 공간이나 더티 메모리 페이지가 사용되는 방식을 조절할 수 있다.

**카프카는 시스템 페이지 캐시를 매우 많이 사용하기 때문에, 가상 메모리 시스템이 디스크로 페이지를 스와핑할 경우 페이지 캐시에 할당할 메모리가 충분하지 않게 된다.**

스왑 메모리 할당 방지하는 방법은 스왑 공간 자체를 할당하지 않는 것. (vm.swappiness = 1)

스왑 메모리보다 페이지 캐시에 많은 메모리를 할당해주느 것이 좋다.

커널이 디스크로 내보내야 할 더티 페이지를 다루는 방식 조정의 이점이 있다.

카프카가 프로듀서의 요청에 빠른 응답을 위해 디스크 I/O 성능에 의존한다.(SSD 등 빠른 디스크가 성능이 좋은 이유)

vm.dirty_background_ratio 설정을 기본값 10보다 작게 잡아주면 좋다.

(5가 적절, 0은 버퍼링이 없이 디스크로 내보내려해서 저장 장치 성능이 튀는걸 발생 시킴)

전체 메모리에서 더티 페이지가 차지할 수 있는 비율을 설정하는 것

커널이 더티 페이지를 강제로 디스크로 동기적으로 내보내기 전 유지할 수 있는 더티페이지의 전체 수를 vm.dirty_ratio로 설정할 수 있다. 60~80이 적절(기본: 20)

하지만 위험이 존재. 밀린 디스크 쓰기 작업 증가, I/O 멈춤이 길어질 수 있다.

이 설정을 올리면 카프카 클러스터 복제 기능을 활성화해서 장애에 안전장치를 할 것을 권장.

카프카는 로그 세그먼트를 저장하고 연결하기 위해 파일 디스크립터를 사용

만약 브로커에 많은 파티션이 저장되어 있을 경우, 브로커는 최소한의 아래 계산식 결과에 수의 파일 디스크립터가 필요하다.

$$
파일 디스크립터 수 = 파티션 수 * (파티션 수/세그먼트 크기) +브로커에 생성된 네트워크 연결 수
$$

vm.max_map_count 설정으로 변경하며 계산에 근거해서 매우 큰 값으로 올려 설정할 것을 권장한다. 400,000~600,000 추천

vm.overcommit_memory=0 권장

0이 아닌 값이면 OS에서 지나치게 많은 메모리를 차지해서 카프카의 최적환경에 메모리가 모자를 수 있다.

3. 네트워킹

대량의 네트워크 트래픽을 발생시키는 애플리케이션을 사용할 때 리눅스 네트워킹 기본 설정값을 조정하는건 흔하다.

각 소켓의 송수신 버퍼에 할당되는 기본/최대 메모리의 양

소켓별 송수신 버퍼의 기본 설정 (권장 131072 = 128KB)

- net.core.wmem_default
- net.core.rmem_default

소켓별 송수신 버퍼의 최대 설정 (권장 2097152 = 2MB)

- net.core.wmem_max
- net_core_rmem_max

TCP 소켓 송수신 버퍼 설정

- net.ipv4.tcp_wmem
- net.ipv4.tcp_rmem
- 4096 65536 2048000 이렇게 설정하면 최소 4KB, 기본 64KB, 최대 2MB가 된다.

그외에 좋은 설정

- net.ipv4.tcp_window_scaling=1 (TCP 윈도우 스케일링 기능을 활성화 시켜 효율적인 데이터 전달)
- net.core.netdev_max_backlog= 1000보다 높게 설정하면 기가비트 네트워크를 사용할 경우 트래픽 급증할 때 도움됨